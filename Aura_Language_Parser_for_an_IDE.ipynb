{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAdVJN3KTqEduL3DZ6Rdl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrikantKGIT/general/blob/main/Aura_Language_Parser_for_an_IDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python script to parse an imaginary language named \"Aura,\" designed to simulate the backend logic of an IDE.**\n",
        "\n",
        "This script demonstrates the core components needed to understand source code:\n",
        "\n",
        "**Token Definitions:** It starts by defining the \"vocabulary\" of the Aura language using regular expressions. This includes keywords (let, print), data types (STRING, NUMBER), and symbols.\n",
        "\n",
        "**Lexer (tokenize function):** This function scans the raw source code and breaks it into a list of Token objects. In an IDE, this list would be used directly to apply syntax highlighting (e.g., color all KEYWORD tokens blue, all STRING tokens green).\n",
        "\n",
        "**Parser:** This class consumes the token list and checks if it follows the language's grammatical rules (e.g., a let statement must have the structure let <variable> = <value>;). It collects any syntax errors it finds.\n",
        "\n",
        "**IDE Simulation:** The analyze_code_for_ide function ties everything together. It takes a string of code, tokenizes it, and then parses it, returning the tokens and a list of errorsâ€”exactly the information an IDE needs to display colored text and red squiggly underlines.\n",
        "\n",
        "**At the end code includes examples with both valid and invalid \"Aura\" code.**"
      ],
      "metadata": {
        "id": "vsRkoPGN6vdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8601KLC6atg",
        "outputId": "92657775-afaa-4b95-f011-e343d79c74bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing GOOD code ---\n",
            "--- Analyzing Code for IDE ---\n",
            "Lexing successful. Tokens generated:\n",
            "  Token(COMMENT, '// This is a valid program', 2, 4)\n",
            "  Token(KEYWORD, 'let', 3, 4)\n",
            "  Token(VARIABLE, 'score', 3, 8)\n",
            "  Token(OPERATOR, '=', 3, 14)\n",
            "  Token(NUMBER, '100', 3, 16)\n",
            "  Token(SEMICOLON, ';', 3, 19)\n",
            "  Token(KEYWORD, 'let', 4, 4)\n",
            "  Token(VARIABLE, 'name', 4, 8)\n",
            "  Token(OPERATOR, '=', 4, 13)\n",
            "  Token(STRING, '\"Player1\"', 4, 15)\n",
            "  Token(SEMICOLON, ';', 4, 24)\n",
            "  Token(KEYWORD, 'print', 5, 4)\n",
            "  Token(VARIABLE, 'score', 5, 10)\n",
            "  Token(SEMICOLON, ';', 5, 15)\n",
            "\n",
            "Parsing complete.\n",
            "\n",
            "Result: Errors found:\n",
            "  - Syntax Error at Line 2, Col 4: Statements must begin with a keyword (let, print).\n",
            "\n",
            "========================================\n",
            "\n",
            "--- Processing BAD code ---\n",
            "--- Analyzing Code for IDE ---\n",
            "Lexing successful. Tokens generated:\n",
            "  Token(COMMENT, '// This program has syntax errors', 2, 4)\n",
            "  Token(KEYWORD, 'let', 3, 4)\n",
            "  Token(VARIABLE, 'x', 3, 8)\n",
            "  Token(NUMBER, '150', 3, 10)\n",
            "  Token(SEMICOLON, ';', 3, 13)\n",
            "  Token(COMMENT, '// Missing '='', 3, 24)\n",
            "  Token(KEYWORD, 'let', 4, 4)\n",
            "  Token(VARIABLE, 'y', 4, 8)\n",
            "  Token(OPERATOR, '=', 4, 10)\n",
            "  Token(STRING, '\"hello\"', 4, 12)\n",
            "  Token(COMMENT, '// Missing semicolon', 4, 24)\n",
            "  Token(KEYWORD, 'print', 5, 4)\n",
            "  Token(VARIABLE, 'z', 5, 10)\n",
            "  Token(SEMICOLON, ';', 5, 11)\n",
            "  Token(VARIABLE, 'show', 6, 4)\n",
            "  Token(VARIABLE, 'z', 6, 9)\n",
            "  Token(SEMICOLON, ';', 6, 10)\n",
            "  Token(COMMENT, '// 'show' is not a valid keyword', 6, 24)\n",
            "\n",
            "Parsing complete.\n",
            "\n",
            "Result: Errors found:\n",
            "  - Syntax Error at Line 2, Col 4: Statements must begin with a keyword (let, print).\n",
            "  - Syntax Error at Line 3, Col 24: Statements must begin with a keyword (let, print).\n",
            "  - Syntax Error at Line 6, Col 4: Statements must begin with a keyword (let, print).\n",
            "  - Syntax Error at Line 6, Col 24: Statements must begin with a keyword (let, print).\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# --- 1. Define the Language's Tokens ---\n",
        "# These are the \"words\" our language understands.\n",
        "TOKEN_SPECIFICATION = [\n",
        "    ('COMMENT',   r'//.*'),          # Comments\n",
        "    ('STRING',    r'\".*?\"'),          # Strings in double quotes\n",
        "    ('NUMBER',    r'\\d+(\\.\\d*)?'),    # Integer or float numbers\n",
        "    ('KEYWORD',   r'\\b(let|print)\\b'),# Keywords: let, print\n",
        "    ('VARIABLE',  r'[A-Za-z_][A-Za-z0-9_]*'), # Variable names\n",
        "    ('OPERATOR',  r'[=+\\-]'),         # Mathematical and assignment operators\n",
        "    ('SEMICOLON', r';'),              # Statement terminator\n",
        "    ('NEWLINE',   r'\\n'),             # Line breaks\n",
        "    ('SKIP',      r'[ \\t]+'),         # Skip over spaces and tabs\n",
        "    ('MISMATCH',  r'.'),              # Any other character is an error\n",
        "]\n",
        "\n",
        "# Create a single regex for tokenizing\n",
        "TOKEN_REGEX = re.compile('|'.join('(?P<%s>%s)' % pair for pair in TOKEN_SPECIFICATION))\n",
        "\n",
        "class Token:\n",
        "    \"\"\"A simple class to hold token information.\"\"\"\n",
        "    def __init__(self, type, value, line, column):\n",
        "        self.type = type\n",
        "        self.value = value\n",
        "        self.line = line\n",
        "        self.column = column\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Token({self.type}, '{self.value}', {self.line}, {self.column})\"\n",
        "\n",
        "# --- 2. The Lexer (Tokenizer) ---\n",
        "# Scans the code and produces a stream of tokens.\n",
        "\n",
        "def tokenize(code):\n",
        "    \"\"\"\n",
        "    Generates a sequence of tokens from a string of code.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    line_num = 1\n",
        "    line_start = 0\n",
        "    for mo in TOKEN_REGEX.finditer(code):\n",
        "        kind = mo.lastgroup\n",
        "        value = mo.group()\n",
        "        column = mo.start() - line_start\n",
        "\n",
        "        if kind == 'NEWLINE':\n",
        "            line_start = mo.end()\n",
        "            line_num += 1\n",
        "            continue # Don't store newline tokens, but track line number\n",
        "        elif kind == 'SKIP':\n",
        "            continue # Ignore whitespace\n",
        "        elif kind == 'MISMATCH':\n",
        "            # This is where an IDE would flag an \"unrecognized character\" error\n",
        "            raise RuntimeError(f'Unexpected character: {value!r} on line {line_num}')\n",
        "\n",
        "        tokens.append(Token(kind, value, line_num, column))\n",
        "    return tokens\n",
        "\n",
        "# --- 3. The Parser ---\n",
        "# Analyzes the token stream to check for correct grammar.\n",
        "\n",
        "class Parser:\n",
        "    \"\"\"\n",
        "    Parses a list of tokens to check for syntax errors.\n",
        "    In a real IDE, this would build an Abstract Syntax Tree (AST).\n",
        "    For simplicity, we'll just validate the structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.pos = 0\n",
        "        self.errors = []\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"Starts the parsing process.\"\"\"\n",
        "        while self.pos < len(self.tokens):\n",
        "            token = self.current_token()\n",
        "            if token.type == 'KEYWORD':\n",
        "                if token.value == 'let':\n",
        "                    self.parse_let_statement()\n",
        "                elif token.value == 'print':\n",
        "                    self.parse_print_statement()\n",
        "            else:\n",
        "                self.error(token, f\"Statements must begin with a keyword (let, print).\")\n",
        "                # Skip to the next potential statement to find more errors\n",
        "                self.advance_to_next_statement()\n",
        "\n",
        "        return self.errors\n",
        "\n",
        "    def parse_let_statement(self):\n",
        "        \"\"\"Parses: let <VAR> = <VALUE>;\"\"\"\n",
        "        self.consume('KEYWORD') # Consume 'let'\n",
        "\n",
        "        # Expect a variable\n",
        "        if self.current_token().type != 'VARIABLE':\n",
        "            self.error(self.current_token(), \"Expected a variable name after 'let'.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume('VARIABLE')\n",
        "\n",
        "        # Expect an equals sign\n",
        "        if self.current_token().type != 'OPERATOR' or self.current_token().value != '=':\n",
        "            self.error(self.current_token(), \"Expected '=' after variable name.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume('OPERATOR')\n",
        "\n",
        "        # Expect a value (Number, String, or another Variable)\n",
        "        if self.current_token().type not in ('NUMBER', 'STRING', 'VARIABLE'):\n",
        "            self.error(self.current_token(), \"Expected a value (number, string, or variable) after '='.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume(self.current_token().type)\n",
        "\n",
        "        # Expect a semicolon\n",
        "        if self.current_token().type != 'SEMICOLON':\n",
        "            self.error(self.current_token(), \"Missing semicolon ';' at the end of the statement.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume('SEMICOLON')\n",
        "\n",
        "    def parse_print_statement(self):\n",
        "        \"\"\"Parses: print <VAR_OR_VALUE>;\"\"\"\n",
        "        self.consume('KEYWORD') # Consume 'print'\n",
        "\n",
        "        # Expect a value to print\n",
        "        if self.current_token().type not in ('VARIABLE', 'NUMBER', 'STRING'):\n",
        "            self.error(self.current_token(), \"Expected a variable, number, or string after 'print'.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume(self.current_token().type)\n",
        "\n",
        "        # Expect a semicolon\n",
        "        if self.current_token().type != 'SEMICOLON':\n",
        "            self.error(self.current_token(), \"Missing semicolon ';' at the end of the statement.\")\n",
        "            self.advance_to_next_statement()\n",
        "            return\n",
        "        self.consume('SEMICOLON')\n",
        "\n",
        "    # --- Helper methods ---\n",
        "    def current_token(self):\n",
        "        if self.pos < len(self.tokens):\n",
        "            return self.tokens[self.pos]\n",
        "        # Return a dummy 'EOF' token to prevent crashes\n",
        "        return Token('EOF', '', -1, -1)\n",
        "\n",
        "    def consume(self, expected_type):\n",
        "        if self.current_token().type == expected_type:\n",
        "            self.pos += 1\n",
        "        else:\n",
        "            self.error(self.current_token(), f\"Expected token type {expected_type} but got {self.current_token().type}.\")\n",
        "\n",
        "    def error(self, token, message):\n",
        "        error_msg = f\"Syntax Error at Line {token.line}, Col {token.column}: {message}\"\n",
        "        self.errors.append(error_msg)\n",
        "\n",
        "    def advance_to_next_statement(self):\n",
        "        \"\"\"Error recovery: find the next semicolon to resume parsing.\"\"\"\n",
        "        while self.pos < len(self.tokens) and self.current_token().type != 'SEMICOLON':\n",
        "            self.pos += 1\n",
        "        if self.pos < len(self.tokens):\n",
        "            self.pos += 1 # Move past the semicolon\n",
        "\n",
        "\n",
        "# --- 4. Example Usage: Simulating an IDE ---\n",
        "\n",
        "def analyze_code_for_ide(code):\n",
        "    \"\"\"\n",
        "    Analyzes a piece of code to provide IDE-like feedback.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (tokens_for_highlighting, list_of_errors).\n",
        "    \"\"\"\n",
        "    print(\"--- Analyzing Code for IDE ---\")\n",
        "\n",
        "    # 1. Lexing (for syntax highlighting)\n",
        "    try:\n",
        "        tokens = tokenize(code)\n",
        "        print(\"Lexing successful. Tokens generated:\")\n",
        "        for t in tokens: print(f\"  {t}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Lexing failed: {e}\")\n",
        "        return [], [str(e)] # Return early if lexing fails\n",
        "\n",
        "    # 2. Parsing (for error checking)\n",
        "    parser = Parser(tokens)\n",
        "    errors = parser.parse()\n",
        "    print(\"\\nParsing complete.\")\n",
        "\n",
        "    return tokens, errors\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example code in our imaginary \"Aura\" language\n",
        "    aura_code_good = \"\"\"\n",
        "    // This is a valid program\n",
        "    let score = 100;\n",
        "    let name = \"Player1\";\n",
        "    print score;\n",
        "    \"\"\"\n",
        "\n",
        "    aura_code_bad = \"\"\"\n",
        "    // This program has syntax errors\n",
        "    let x 150;          // Missing '='\n",
        "    let y = \"hello\"     // Missing semicolon\n",
        "    print z;\n",
        "    show z;             // 'show' is not a valid keyword\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"--- Processing GOOD code ---\")\n",
        "    good_tokens, good_errors = analyze_code_for_ide(aura_code_good)\n",
        "    if not good_errors:\n",
        "        print(\"\\nResult: No syntax errors found. Ready for highlighting!\")\n",
        "    else:\n",
        "        print(\"\\nResult: Errors found:\")\n",
        "        for err in good_errors: print(f\"  - {err}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "    print(\"--- Processing BAD code ---\")\n",
        "    bad_tokens, bad_errors = analyze_code_for_ide(aura_code_bad)\n",
        "    if not bad_errors:\n",
        "        print(\"\\nResult: No syntax errors found.\")\n",
        "    else:\n",
        "        print(\"\\nResult: Errors found:\")\n",
        "        for err in bad_errors: print(f\"  - {err}\")\n"
      ]
    }
  ]
}