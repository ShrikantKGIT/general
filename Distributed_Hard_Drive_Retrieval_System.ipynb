{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+a0haViGcfU0gymU/re6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrikantKGIT/general/blob/main/Distributed_Hard_Drive_Retrieval_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple distributed hard drive retrieval system in Python.**\n",
        "\n",
        "This system will simulate a cluster of independent storage nodes (hard drives) that work together. A central manager will keep track of where data chunks are stored, and a client will be able to read and write files, with the system automatically handling the failure of a node by retrieving data from a replica.\n",
        "\n",
        "The key concepts demonstrated are:\n",
        "\n",
        "**Sharding:** Data is split into smaller chunks.\n",
        "\n",
        "**Replication:** Each chunk is stored on multiple nodes for redundancy.\n",
        "\n",
        "**Fault Tolerance:** The system can still retrieve a file even if one of the nodes holding its data goes offline."
      ],
      "metadata": {
        "id": "s86S4tIBuJc2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKYgj96Wth9y",
        "outputId": "f3ded232-af56-4f03-a61d-ef30f0351e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 'Node-0' initialized.\n",
            "Node 'Node-1' initialized.\n",
            "Node 'Node-2' initialized.\n",
            "Node 'Node-3' initialized.\n",
            "Node 'Node-0' registered with the cluster.\n",
            "Node 'Node-1' registered with the cluster.\n",
            "Node 'Node-2' registered with the cluster.\n",
            "Node 'Node-3' registered with the cluster.\n",
            "\n",
            "Writing file 'mydoc.txt' to the cluster...\n",
            "  - Wrote chunk 'a1cd542a...' to nodes ['Node-2', 'Node-3']\n",
            "File 'mydoc.txt' written successfully.\n",
            "\n",
            "Reading file 'mydoc.txt' from the cluster...\n",
            "  - Read chunk 'a1cd542a...' successfully from Node 'Node-2'\n",
            "\n",
            "Initial read successful. Content: 'This is a very important document about distributed systems. It needs to be stored reliably and should survive the failure of a single node.'\n",
            "\n",
            "==================================================\n",
            "\n",
            "CRITICAL: Node 'Node-2' has gone offline!\n",
            "\n",
            "Reading file 'mydoc.txt' from the cluster...\n",
            "  - Read chunk 'a1cd542a...' successfully from Node 'Node-3'\n",
            "\n",
            "--- Final Result ---\n",
            "Original data:  'This is a very important document about distributed systems. It needs to be stored reliably and should survive the failure of a single node.'\n",
            "Recovered data: 'This is a very important document about distributed systems. It needs to be stored reliably and should survive the failure of a single node.'\n",
            "\n",
            "SUCCESS: Data was automatically recovered from a replica node!\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "import random\n",
        "\n",
        "# --- Part 1: The Individual Storage Node (Simulated Hard Drive) ---\n",
        "class StorageNode:\n",
        "    \"\"\"\n",
        "    Simulates a single, independent hard drive in the distributed network.\n",
        "    It's a simple key-value store where the key is the chunk hash.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_id):\n",
        "        self.node_id = node_id\n",
        "        self.storage = {}  # In-memory dictionary to store data chunks\n",
        "        self.is_online = True\n",
        "        print(f\"Node '{self.node_id}' initialized.\")\n",
        "\n",
        "    def write_chunk(self, chunk_hash, data):\n",
        "        \"\"\"Writes a data chunk to the node's storage.\"\"\"\n",
        "        if not self.is_online:\n",
        "            raise ConnectionError(f\"Node '{self.node_id}' is offline.\")\n",
        "        self.storage[chunk_hash] = data\n",
        "        # print(f\"DEBUG: Node '{self.node_id}' stored chunk '{chunk_hash[:8]}...'\")\n",
        "\n",
        "    def read_chunk(self, chunk_hash):\n",
        "        \"\"\"Reads a data chunk from the node's storage.\"\"\"\n",
        "        if not self.is_online:\n",
        "            raise ConnectionError(f\"Node '{self.node_id}' is offline.\")\n",
        "        if chunk_hash not in self.storage:\n",
        "            raise FileNotFoundError(f\"Chunk not found on Node '{self.node_id}'.\")\n",
        "        return self.storage[chunk_hash]\n",
        "\n",
        "    def unplug(self):\n",
        "        \"\"\"Simulates the node going offline.\"\"\"\n",
        "        self.is_online = False\n",
        "        print(f\"CRITICAL: Node '{self.node_id}' has gone offline!\")\n",
        "\n",
        "    def plug_in(self):\n",
        "        \"\"\"Brings the node back online.\"\"\"\n",
        "        self.is_online = True\n",
        "        print(f\"INFO: Node '{self.node_id}' is back online.\")\n",
        "\n",
        "# --- Part 2: The Cluster Manager (Keeps track of everything) ---\n",
        "class ClusterManager:\n",
        "    \"\"\"\n",
        "    Manages the cluster of storage nodes. It knows which nodes are online\n",
        "    and where each file chunk and its replicas are stored.\n",
        "    \"\"\"\n",
        "    def __init__(self, replication_factor=2):\n",
        "        self.nodes = {}  # node_id -> node_instance\n",
        "        self.chunk_map = {}  # chunk_hash -> [node_id_1, node_id_2, ...]\n",
        "        self.file_map = {} # filename -> [chunk_hash_1, chunk_hash_2, ...]\n",
        "        self.replication_factor = replication_factor\n",
        "\n",
        "    def register_node(self, node):\n",
        "        \"\"\"Adds a new storage node to the cluster.\"\"\"\n",
        "        self.nodes[node.node_id] = node\n",
        "        print(f\"Node '{node.node_id}' registered with the cluster.\")\n",
        "\n",
        "    def get_storage_nodes_for_chunk(self):\n",
        "        \"\"\"\n",
        "        Selects a set of online nodes to store a chunk and its replicas.\n",
        "        Ensures replicas are on different nodes.\n",
        "        \"\"\"\n",
        "        online_nodes = [n for n in self.nodes.values() if n.is_online]\n",
        "        if len(online_nodes) < self.replication_factor:\n",
        "            raise ConnectionError(\"Not enough online nodes to meet replication factor.\")\n",
        "\n",
        "        # Return a random sample of nodes\n",
        "        return random.sample(online_nodes, self.replication_factor)\n",
        "\n",
        "    def add_file_to_index(self, filename, chunk_hashes, chunk_locations):\n",
        "        \"\"\"Updates the central index with file and chunk information.\"\"\"\n",
        "        self.file_map[filename] = chunk_hashes\n",
        "        for chunk_hash, nodes in chunk_locations.items():\n",
        "            self.chunk_map[chunk_hash] = [node.node_id for node in nodes]\n",
        "\n",
        "    def get_file_chunk_hashes(self, filename):\n",
        "        \"\"\"Returns the list of chunk hashes for a given file.\"\"\"\n",
        "        return self.file_map.get(filename)\n",
        "\n",
        "    def get_chunk_locations(self, chunk_hash):\n",
        "        \"\"\"Returns a list of node instances where a chunk can be found.\"\"\"\n",
        "        node_ids = self.chunk_map.get(chunk_hash, [])\n",
        "        # Return node instances, prioritizing online ones\n",
        "        online_nodes = [self.nodes[nid] for nid in node_ids if self.nodes[nid].is_online]\n",
        "        offline_nodes = [self.nodes[nid] for nid in node_ids if not self.nodes[nid].is_online]\n",
        "        return online_nodes + offline_nodes # Put online nodes first\n",
        "\n",
        "# --- Part 3: The Client (API for reading and writing files) ---\n",
        "class Client:\n",
        "    \"\"\"\n",
        "    Provides a simple API to interact with the distributed storage system.\n",
        "    \"\"\"\n",
        "    CHUNK_SIZE = 1024 # 1 KB chunks\n",
        "\n",
        "    def __init__(self, cluster_manager):\n",
        "        self.manager = cluster_manager\n",
        "\n",
        "    def write(self, filename, data):\n",
        "        \"\"\"\n",
        "        Writes a file to the distributed system.\n",
        "        1. Splits data into chunks.\n",
        "        2. For each chunk, asks the manager for nodes to store it on.\n",
        "        3. Writes the chunk to the selected primary and replica nodes.\n",
        "        \"\"\"\n",
        "        print(f\"\\nWriting file '{filename}' to the cluster...\")\n",
        "        content_bytes = data.encode('utf-8')\n",
        "        chunk_hashes = []\n",
        "        chunk_locations = {}\n",
        "\n",
        "        # 1. Split data into chunks\n",
        "        for i in range(0, len(content_bytes), self.CHUNK_SIZE):\n",
        "            chunk = content_bytes[i:i+self.CHUNK_SIZE]\n",
        "            chunk_hash = hashlib.sha256(chunk).hexdigest()\n",
        "            chunk_hashes.append(chunk_hash)\n",
        "\n",
        "            # 2. Get nodes for this chunk\n",
        "            try:\n",
        "                nodes_for_chunk = self.manager.get_storage_nodes_for_chunk()\n",
        "                chunk_locations[chunk_hash] = nodes_for_chunk\n",
        "\n",
        "                # 3. Write chunk to all assigned nodes (primary + replicas)\n",
        "                for node in nodes_for_chunk:\n",
        "                    node.write_chunk(chunk_hash, chunk)\n",
        "                print(f\"  - Wrote chunk '{chunk_hash[:8]}...' to nodes {[n.node_id for n in nodes_for_chunk]}\")\n",
        "\n",
        "            except ConnectionError as e:\n",
        "                print(f\"Error writing chunk: {e}\")\n",
        "                return False\n",
        "\n",
        "        # 4. Update the central index\n",
        "        self.manager.add_file_to_index(filename, chunk_hashes, chunk_locations)\n",
        "        print(f\"File '{filename}' written successfully.\")\n",
        "        return True\n",
        "\n",
        "    def read(self, filename):\n",
        "        \"\"\"\n",
        "        Reads a file from the distributed system.\n",
        "        1. Gets the list of chunk hashes for the file.\n",
        "        2. For each chunk, gets its possible locations.\n",
        "        3. Tries to read from the first location. If it fails, tries the next (replica).\n",
        "        4. Reassembles the chunks into the original file data.\n",
        "        \"\"\"\n",
        "        print(f\"\\nReading file '{filename}' from the cluster...\")\n",
        "        chunk_hashes = self.manager.get_file_chunk_hashes(filename)\n",
        "        if not chunk_hashes:\n",
        "            raise FileNotFoundError(f\"File '{filename}' not found in the cluster index.\")\n",
        "\n",
        "        file_data = b''\n",
        "        for chunk_hash in chunk_hashes:\n",
        "            locations = self.manager.get_chunk_locations(chunk_hash)\n",
        "            if not locations:\n",
        "                raise IOError(f\"FATAL: All nodes for chunk '{chunk_hash[:8]}...' are offline. Data is lost.\")\n",
        "\n",
        "            chunk_data = None\n",
        "            for node in locations:\n",
        "                try:\n",
        "                    chunk_data = node.read_chunk(chunk_hash)\n",
        "                    print(f\"  - Read chunk '{chunk_hash[:8]}...' successfully from Node '{node.node_id}'\")\n",
        "                    break # Success, move to the next chunk\n",
        "                except (ConnectionError, FileNotFoundError) as e:\n",
        "                    print(f\"  - Could not read chunk '{chunk_hash[:8]}...' from Node '{node.node_id}': {e}. Trying replica...\")\n",
        "\n",
        "            if chunk_data is None:\n",
        "                raise IOError(f\"FATAL: Failed to read chunk '{chunk_hash[:8]}...' from any of its locations.\")\n",
        "\n",
        "            file_data += chunk_data\n",
        "\n",
        "        return file_data.decode('utf-8')\n",
        "\n",
        "# --- Part 4: Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Set up the cluster\n",
        "    manager = ClusterManager(replication_factor=2)\n",
        "    nodes = [StorageNode(f\"Node-{i}\") for i in range(4)]\n",
        "    for n in nodes:\n",
        "        manager.register_node(n)\n",
        "\n",
        "    # 2. Create a client to interact with the cluster\n",
        "    client = Client(manager)\n",
        "\n",
        "    # 3. Write a file\n",
        "    my_document = \"This is a very important document about distributed systems. It needs to be stored reliably and should survive the failure of a single node.\"\n",
        "    client.write(\"mydoc.txt\", my_document)\n",
        "\n",
        "    # 4. Read it back to verify\n",
        "    retrieved_doc = client.read(\"mydoc.txt\")\n",
        "    print(f\"\\nInitial read successful. Content: '{retrieved_doc}'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # 5. --- SIMULATE A NODE FAILURE ---\n",
        "    # Find a node that holds a piece of our file and take it offline\n",
        "    first_chunk_hash = manager.get_file_chunk_hashes(\"mydoc.txt\")[0]\n",
        "    nodes_with_chunk = manager.chunk_map[first_chunk_hash]\n",
        "    node_to_unplug_id = nodes_with_chunk[0]\n",
        "    manager.nodes[node_to_unplug_id].unplug()\n",
        "\n",
        "    # 6. Try to read the file again. The system should automatically use the replica.\n",
        "    try:\n",
        "        recovered_doc = client.read(\"mydoc.txt\")\n",
        "        print(\"\\n--- Final Result ---\")\n",
        "        print(f\"Original data:  '{my_document}'\")\n",
        "        print(f\"Recovered data: '{recovered_doc}'\")\n",
        "\n",
        "        if my_document == recovered_doc:\n",
        "            print(\"\\nSUCCESS: Data was automatically recovered from a replica node!\")\n",
        "        else:\n",
        "            print(\"\\nFAILURE: Recovered data does not match the original.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during recovery: {e}\")\n"
      ]
    }
  ]
}